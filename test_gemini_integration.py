#!/usr/bin/env python3
"""
Script de prueba para el provider de Gemini en GraphRAG.
Este script verifica que la integraci√≥n con Gemini funcione correctamente.
"""

import asyncio
import os
import sys
from pathlib import Path

# A√±adir el directorio ra√≠z al path para importar GraphRAG
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from graphrag.config.enums import ModelType
from graphrag.config.models.language_model_config import LanguageModelConfig
from graphrag.language_model.providers.gemini.models import (
    GeminiChatProvider,
    GeminiEmbeddingProvider,
)
from graphrag.language_model.factory import ModelFactory


async def test_gemini_chat():
    """Prueba el modelo de chat de Gemini."""
    print("üß™ Probando Gemini Chat...")
    
    try:
        # Configuraci√≥n del modelo
        config = LanguageModelConfig(
            type="gemini_chat",
            api_key="AIzaSyD2emQC2huUXTcykYTM2uSgVxZPuAH_dqc",
            model="models/gemini-1.5-flash",  # Modelo estable y disponible
            temperature=0.7,
            max_tokens=1000,
            encoding_model="cl100k_base",  # Evitar problemas con tiktoken
        )
        
        # Crear instancia directa
        chat_provider = GeminiChatProvider(
            name="test_gemini_chat",
            config=config,
        )
        
        # Prueba de chat simple
        prompt = "Hola, ¬øpuedes explicarme brevemente qu√© es GraphRAG?"
        print(f"üìù Prompt: {prompt}")
        
        response = await chat_provider.achat(prompt)
        print(f"‚úÖ Respuesta: {response.output.content[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en chat de Gemini: {e}")
        return False


async def test_gemini_embedding():
    """Prueba el modelo de embeddings de Gemini."""
    print("\nüß™ Probando Gemini Embeddings...")
    
    try:
        # Configuraci√≥n del modelo
        config = LanguageModelConfig(
            type="gemini_embedding",
            api_key="AIzaSyD2emQC2huUXTcykYTM2uSgVxZPuAH_dqc",
            model="models/text-embedding-004",
            encoding_model="cl100k_base",  # Evitar problemas con tiktoken
        )
        
        # Crear instancia directa
        embedding_provider = GeminiEmbeddingProvider(
            name="test_gemini_embedding",
            config=config,
        )
        
        # Prueba de embedding simple
        text = "GraphRAG es una t√©cnica que combina grafos de conocimiento con generaci√≥n aumentada por recuperaci√≥n."
        print(f"üìù Texto: {text}")
        
        embedding = await embedding_provider.aembed(text)
        print(f"‚úÖ Embedding generado: dimensiones = {len(embedding)}")
        print(f"‚úÖ Primeros 5 valores: {embedding[:5]}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en embeddings de Gemini: {e}")
        return False


def test_model_factory():
    """Prueba que la factory puede crear modelos de Gemini."""
    print("\nüß™ Probando Model Factory...")
    
    try:
        # Verificar que los modelos est√°n registrados
        chat_models = ModelFactory.get_chat_models()
        embedding_models = ModelFactory.get_embedding_models()
        
        print(f"‚úÖ Modelos de chat disponibles: {chat_models}")
        print(f"‚úÖ Modelos de embedding disponibles: {embedding_models}")
        
        # Verificar que Gemini est√° incluido
        assert ModelType.GeminiChat.value in chat_models
        assert ModelType.GeminiEmbedding.value in embedding_models
        
        print("‚úÖ Gemini est√° correctamente registrado en la factory")
        return True
        
    except Exception as e:
        print(f"‚ùå Error en factory: {e}")
        return False


async def test_full_integration():
    """Prueba la integraci√≥n completa usando la factory."""
    print("\nüß™ Probando integraci√≥n completa...")
    
    try:
        # Crear modelo de chat usando la factory
        chat_model = ModelFactory.create_chat_model(
            ModelType.GeminiChat.value,
            name="test_chat",
            config=LanguageModelConfig(
                type="gemini_chat",
                api_key="AIzaSyD2emQC2huUXTcykYTM2uSgVxZPuAH_dqc",
                model="models/gemini-1.5-flash",  # Modelo estable y disponible
                encoding_model="cl100k_base",
            ),
        )
        
        # Crear modelo de embedding usando la factory
        embedding_model = ModelFactory.create_embedding_model(
            ModelType.GeminiEmbedding.value,
            name="test_embedding",
            config=LanguageModelConfig(
                type="gemini_embedding",
                api_key="AIzaSyD2emQC2huUXTcykYTM2uSgVxZPuAH_dqc",
                model="models/text-embedding-004",
                encoding_model="cl100k_base",
            ),
        )
        
        # Prueba r√°pida
        response = await chat_model.achat("¬øQu√© es IA?")
        embedding = await embedding_model.aembed("Inteligencia Artificial")
        
        print("‚úÖ Integraci√≥n completa funcionando correctamente")
        print(f"‚úÖ Chat respuesta: {response.output.content[:100]}...")
        print(f"‚úÖ Embedding dimensiones: {len(embedding)}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en integraci√≥n completa: {e}")
        return False


async def main():
    """Funci√≥n principal que ejecuta todas las pruebas."""
    print("üöÄ Iniciando pruebas del provider de Gemini para GraphRAG\n")
    
    results = []
    
    # Ejecutar todas las pruebas
    results.append(test_model_factory())
    results.append(await test_gemini_chat())
    results.append(await test_gemini_embedding())
    results.append(await test_full_integration())
    
    # Resumen
    print("\n" + "="*50)
    print("üìä RESUMEN DE PRUEBAS")
    print("="*50)
    
    passed = sum(results)
    total = len(results)
    
    print(f"‚úÖ Pruebas exitosas: {passed}/{total}")
    
    if passed == total:
        print("üéâ ¬°Todas las pruebas pasaron! Gemini est√° correctamente integrado.")
        return 0
    else:
        print("‚ö†Ô∏è  Algunas pruebas fallaron. Revisa los errores anteriores.")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
